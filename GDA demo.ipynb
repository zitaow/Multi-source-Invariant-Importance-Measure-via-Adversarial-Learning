{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1808fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "603a1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_simplex(v):\n",
    "    if torch.all(v >= 0) and torch.isclose(torch.sum(v), torch.tensor(1.0)):\n",
    "        return v\n",
    "\n",
    "    u, indices = torch.sort(v, descending=True)\n",
    "    cssv = torch.cumsum(u, dim=0)\n",
    "    rho = torch.where(u - (cssv - 1) / (torch.arange(1, len(v)+1, device=v.device, dtype=v.dtype)) > 0)[0].max()\n",
    "    tau = (cssv[rho] - 1) / (rho + 1)\n",
    "\n",
    "    w = torch.maximum(v - tau, torch.tensor(0.0))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff6b2ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def logistic(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def logit(p):\n",
    "    return np.log(p / (1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c8ba238",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000 #sample size\n",
    "p = 3 #covariate length\n",
    "k = 3 #size size\n",
    "beta_1 = np.array([0.9,0.4,0.4])\n",
    "beta_2 = np.array([0.4,0.9,0.4])\n",
    "beta_3 = np.array([0.4,0.4,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0531c60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7d7e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0.2001, 0.3939, 0.4060]) -0.08286803960800171 -0.11398577690124512 -0.05364882946014404\n",
      "100 tensor([0.2962, 0.3749, 0.3289]) -0.12488830089569092 -0.12698894739151 -0.13000208139419556\n",
      "200 tensor([0.3230, 0.3770, 0.3000]) -0.12663298845291138 -0.12717992067337036 -0.12809354066848755\n",
      "300 tensor([0.3306, 0.3789, 0.2905]) -0.1270904541015625 -0.12721580266952515 -0.1275426149368286\n",
      "400 tensor([0.3328, 0.3798, 0.2874]) -0.12721657752990723 -0.12724590301513672 -0.12735843658447266\n",
      "500 tensor([0.3335, 0.3801, 0.2864]) -0.12725287675857544 -0.12725985050201416 -0.12729781866073608\n",
      "600 tensor([0.3337, 0.3803, 0.2860]) -0.127263605594635 -0.12726527452468872 -0.12727802991867065\n",
      "700 tensor([0.3338, 0.3803, 0.2859]) -0.12726688385009766 -0.12726730108261108 -0.12727147340774536\n",
      "800 tensor([0.3338, 0.3803, 0.2859]) -0.1272679567337036 -0.1272679567337036 -0.12726932764053345\n",
      "900 tensor([0.3338, 0.3803, 0.2859]) -0.12726819515228271 -0.1272682547569275 -0.12726867198944092\n",
      "1000 tensor([0.3338, 0.3803, 0.2859]) -0.12726831436157227 -0.12726837396621704 -0.1272684931755066\n",
      "1100 tensor([0.3338, 0.3803, 0.2859]) -0.12726837396621704 -0.12726837396621704 -0.12726843357086182\n",
      "1200 tensor([0.3338, 0.3803, 0.2859]) -0.12726837396621704 -0.12726837396621704 -0.12726843357086182\n",
      "1300 tensor([0.3338, 0.3803, 0.2859]) -0.12726837396621704 -0.12726837396621704 -0.12726843357086182\n",
      "1400 tensor([0.3338, 0.3803, 0.2859]) -0.12726837396621704 -0.12726837396621704 -0.12726843357086182\n",
      "1500 tensor([0.3338, 0.3803, 0.2859]) -0.12726837396621704 -0.12726837396621704 -0.12726843357086182\n",
      "1600 tensor([0.3338, 0.3803, 0.2859]) -0.12726837396621704 -0.12726837396621704 -0.12726843357086182\n",
      "1700 tensor([0.3338, 0.3803, 0.2859]) -0.12726837396621704 -0.12726837396621704 -0.12726843357086182\n",
      "1800 tensor([0.3338, 0.3803, 0.2859]) -0.12726837396621704 -0.12726837396621704 -0.12726843357086182\n",
      "1900 tensor([0.3338, 0.3803, 0.2859]) -0.12726837396621704 -0.12726837396621704 -0.12726843357086182\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    X_1 = np.random.uniform(-2, 2, (n, p))\n",
    "    X_2 = np.random.uniform(-2, 2, (n, p))\n",
    "    X_3 = np.random.uniform(-2, 2, (n, p))\n",
    "    \n",
    "    logits_1 = X_1.dot(beta_1)\n",
    "    probs_1 = sigmoid(logits_1)\n",
    "    y_1 = np.random.binomial(1, probs_1)\n",
    "    \n",
    "    logits_2 = X_2.dot(beta_2)\n",
    "    probs_2 = sigmoid(logits_2)\n",
    "    y_2 = np.random.binomial(1, probs_2)\n",
    "\n",
    "    logits_3 = X_3.dot(beta_3)\n",
    "    probs_3 = sigmoid(logits_3)\n",
    "    y_3 = np.random.binomial(1, probs_3)\n",
    "\n",
    "    p1_bar = np.mean(y_1)\n",
    "    p2_bar = np.mean(y_2)\n",
    "    p3_bar = np.mean(y_3)\n",
    "\n",
    "    null_1 = np.mean(np.log(p1_bar)*y_1 + np.log(1-p1_bar)*(1-y_1))\n",
    "    null_2 = np.mean(np.log(p2_bar)*y_2 + np.log(1-p2_bar)*(1-y_2))\n",
    "    null_3 = np.mean(np.log(p3_bar)*y_3 + np.log(1-p3_bar)*(1-y_3))\n",
    "    \n",
    "    X1_tensor = torch.tensor(X_1, dtype=torch.float32)\n",
    "    X2_tensor = torch.tensor(X_2, dtype=torch.float32)\n",
    "    X3_tensor = torch.tensor(X_3, dtype=torch.float32)\n",
    "\n",
    "    y1_tensor = torch.tensor(y_1, dtype=torch.float32)\n",
    "    y2_tensor = torch.tensor(y_2, dtype=torch.float32)\n",
    "    y3_tensor = torch.tensor(y_3, dtype=torch.float32)\n",
    "\n",
    "    # Initialize the parameter vector and weight vector\n",
    "    a = torch.randn((p, 1), requires_grad=True)\n",
    "    q = nn.Parameter(torch.tensor([0.233, 0.433, 0.433], requires_grad=True))\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    optimizer_mlp = optim.SGD([a], lr=0.1, momentum=0.9)\n",
    "    optimizer_q = optim.SGD([q], lr=0.2)\n",
    "\n",
    "    for epoch in range(2000):\n",
    "        optimizer_mlp.zero_grad()\n",
    "\n",
    "        # Compute logits manually\n",
    "        logits1 = X1_tensor @ a\n",
    "        logits2 = X2_tensor @ a\n",
    "        logits3 = X3_tensor @ a\n",
    "\n",
    "        # Apply sigmoid function\n",
    "        outputs1 = torch.sigmoid(logits1).squeeze(1)\n",
    "        outputs2 = torch.sigmoid(logits2).squeeze(1)\n",
    "        outputs3 = torch.sigmoid(logits3).squeeze(1)\n",
    "\n",
    "        # Compute the losses for all datasets\n",
    "        loss1 = criterion(outputs1, y1_tensor) + null_1\n",
    "        loss2 = criterion(outputs2, y2_tensor) + null_2\n",
    "        loss3 = criterion(outputs3, y3_tensor) + null_3\n",
    "\n",
    "        # MLP update\n",
    "        weighted_loss_mlp = q[0] * loss1 + q[1] * loss2 + q[2] * loss3# + torch.norm(a,1)/n if you need regularization\n",
    "        weighted_loss_mlp.backward()\n",
    "        optimizer_mlp.step()\n",
    "\n",
    "        # q-update\n",
    "        optimizer_q.zero_grad()\n",
    "        # Detach the losses from the graph used for MLP parameters update\n",
    "        loss1_detached = loss1.detach()\n",
    "        loss2_detached = loss2.detach()\n",
    "        loss3_detached = loss3.detach()\n",
    "        weighted_loss_q = q[0] * loss1_detached + q[1] * loss2_detached + q[2] * loss3_detached# + torch.norm(a,1)/n\n",
    "        (-weighted_loss_q).backward()  # Gradient ascent for q\n",
    "        optimizer_q.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q[:] = project_simplex(q)\n",
    "            \n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch ,q.data, loss1.item(),loss2.item(),loss3.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
